#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Created on Tue Dec 05 2023
@author: rivera
"""
from sys import argv as args
import os
from csv_diff import load_csv, compare

TEST_FILES: list = ['test01.csv',
                    'test02.csv',
                    'test03.csv',
                    'test04.csv',
                    'test05.csv']
REQUIRED_FILES: list = ['song_analyzer', 'before_2020s.csv', 'during_2020s.csv']
TESTER_PROGRAM_NAME: str = 'tester'
PROGRAM_ARGS: str = '<question(e.g.,1,2,3,4,5)>'
REMOVE_FILES: bool = True
USAGE_MSG: str = f'Usage: ./{TESTER_PROGRAM_NAME} {PROGRAM_ARGS} or ./{TESTER_PROGRAM_NAME}'


def is_empty_file(file_path):
    """Determines whether a file is empty or not.
            Parameters
            ----------
                file_path : str, required
                    The file path.
            Returns
            -------
                bool
    """
    return os.path.getsize(file_path) == 0


def required_files_exist() -> bool:
    """Determines whether there are missing files.
            Returns
            -------
                bool
    """
    exist: bool = True
    for file in REQUIRED_FILES + TEST_FILES:
        if not os.path.isfile(file):
            exist = False
            break
    return exist


def print_message(is_error: bool, message: str) -> None:
    """Prints a message to stdout.
            Parameters
            ----------
                is_error : bool, required
                    Indicates whether the message is an error.
                message : str, required
                    The message to be printed out.
    """
    message_type: str = 'ERROR' if is_error else 'INFO'
    print(f'[{TESTER_PROGRAM_NAME}] ({message_type}): {message}')


def generate_execution_commands(question: str) -> list:
    """Generates the execution commands for the tests.
            Parameters
            ----------
                question : str, required
                    The question for the commands to be generated. None if all test cases should be generated.
            Returns
            -------
                list
                    A list with all the commands generated.
    """
    commands: list = []
    commands.append('./song_analyzer --question=1 --data=before_2020s.csv')
    commands.append('./song_analyzer --question=2 --data=during_2020s.csv')
    commands.append('./song_analyzer --question=3 --data=before_2020s.csv')
    commands.append('./song_analyzer --question=4 --data=during_2020s.csv')
    commands.append('./song_analyzer --question=5 --data=during_2020s.csv')
    number: int = -1
    if question is not None:
        number = int(question) - 1
    commands = [commands[number]] if number >= 0 else commands
    return commands


def validate_tests(execution_commands: list, question: str) -> None:
    """Generates the execution commands for the tests.
            Parameters
            ----------
                execution_commands : list, required
                    The generated commands.
                question : str, required
                    The question for the commands to be generated. None if all test cases should be generated.
    """
    separator: str = '----------------------------------------'
    print_message(is_error=False, message=f'Tests to run: {len(execution_commands)}')
    tests_passed: int = 0
    result = ''
    for i in range(len(execution_commands)):
        test: int = int(question) if question is not None else i + 1
        print_message(is_error=False, message=f'|Test {test}|' + separator)
        command: str = execution_commands[i]
        required: list = [f'output.csv']
        # delete existing files
        for required_file in required:
            if os.path.isfile(required_file) and REMOVE_FILES:
                os.remove(required_file)
        test_pass: bool = True
        test_error: bool = False
        print_message(is_error=False, message=f'Attempting: {command}')
        # execute command
        os.system(command=command)
        # validate generated files (csv)
        if not os.path.isfile(required[0]):
            print_message(is_error=False, message=f'song_analyzer should generate {required[0]} for this test.')
            test_pass = False
            test_error = True
        else:
            # read csvs
            produced_file: str = required[0]
            expected_file: str = f'test0{test}.csv'
            # check whether the files are empty
            if is_empty_file(produced_file) or is_empty_file(expected_file):
                print_message(is_error=True, message=f'{produced_file} and {expected_file} can not be empty.')
                test_pass = False
                test_error = True
            else:
                # read csvs
                produced_data = load_csv(open(produced_file, encoding='utf-8-sig'))
                expected_data = load_csv(open(expected_file, encoding='utf-8-sig'))
                # check rows of files
                no_rows_produced: bool = True if not produced_data else False
                no_rows_expected: bool = True if not expected_data else False
                if no_rows_produced or no_rows_expected:
                    if test != 1:
                        print_message(is_error=True, message=f'{produced_file} and {expected_file} should not be empty for this test.')
                        test_pass = False
                        test_error = True
                    else:
                        # compare lines
                        p_line1, p_line2, e_line1, e_line2 = None, None, None, None
                        with open(produced_file, 'r') as p_file:
                            p_line1 = p_file.readline()
                            p_line2 = p_file.readline()
                        with open(expected_file, 'r') as e_file:
                            e_line1 = e_file.readline()
                            e_line2 = e_file.readline() 
                        if p_line1 != e_line1 or p_line2 != e_line2:
                            test_pass = False
                else:
                    # obtain the differences
                    result = compare(produced_data, expected_data)
                    # compare
                    if len(result['added']) > 0 or len(result['removed']) > 0 or len(result['changed']) > 0 or len(result['columns_added']) > 0 or len(result['columns_removed']) > 0:
                        test_pass = False
        print_message(is_error=False, message=f'TEST PASSED: {test_pass}')
        if not test_pass and not test_error:
            print_message(is_error=False, message=f'DIFFERENCES: {result}')
        if test_pass:
            tests_passed += 1          
    print_message(is_error=False, message=separator + '--------')
    print_message(is_error=False, message=f'TESTS PASSED: {tests_passed}/{len(execution_commands)}')


def main():
    """Main entry point of the program."""
    if len(args) - 1 > len(PROGRAM_ARGS.split(" ")):
        print_message(is_error=True, message=USAGE_MSG)
    else:
        question: str = None
        if len(args) != 1:
            question = args[1]
        # validate required files
        if not required_files_exist():
            print_message(is_error=True, message=f'Required files: {REQUIRED_FILES + TEST_FILES}')
        else:
            # validate args
            valid_args: bool = True
            try:
                if question is not None:
                    question_int: int = int(question)
                    if question_int not in [1, 2, 3, 4, 5]:
                        valid_args = False
            except ValueError:
                valid_args = False
            if valid_args:
                commands: list = generate_execution_commands(question=question)
                validate_tests(execution_commands=commands, question=question)
            else:
                print_message(is_error=True, message=USAGE_MSG)


if __name__ == '__main__':
    main()
